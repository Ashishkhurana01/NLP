{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "# for all NLP related operations on text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# To consume Twitter's API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler \n",
    "\n",
    "# To identify the sentiment of text\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "# ignoring all the warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# downloading stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('brown')\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# for showing all the plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterClient(object): \n",
    "    def __init__(self): \n",
    "        #Initialization method. \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            auth = OAuthHandler('MWagHIMZgBCE8ExJw4ZEhBeqd', 'pHoXxG9mXVDlLInne7xajFJ8WdyPKhuXdAwJOnYu9qUOT0Go8U') \n",
    "            # set access token and secret \n",
    "            auth.set_access_token('18837312-7UAgD8xarB9RgUKv263ftqGKnYkSIHAgjzVvf7K1f', 'pjc3MOSDmCXz3mAFt28gfjWdQqNVqcSUzA4BhVChNFQqP') \n",
    "            # create tweepy API object to fetch tweets \n",
    "            # add hyper parameter 'proxy' if executing from behind proxy \"proxy='http://172.22.218.218:8085'\"\n",
    "            self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "            \n",
    "        except tweepy.TweepError as e:\n",
    "            print(f\"Error: Twitter Authentication Failed - \\n{str(e)}\")\n",
    "\n",
    "    def get_tweets(self, query, maxTweets = 1000):\n",
    "        #Function to fetch tweets. \n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "        sinceId = None\n",
    "        max_id = -1\n",
    "        tweetCount = 0\n",
    "        tweetsPerQry = 100\n",
    "\n",
    "        while tweetCount < maxTweets:\n",
    "            try:\n",
    "                if (max_id <= 0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = self.api.search(q=query, geo = '43.651070,-79.347015,250km', lang='en', count=tweetsPerQry)\n",
    "                    else:\n",
    "                        new_tweets = self.api.search(q=query, count=tweetsPerQry,geo = '43.651070,-79.347015,250km', lang='en',\n",
    "                                                since_id=sinceId)\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = self.api.search(q=query, count=tweetsPerQry,geo = '43.651070,-79.347015,250km', lang='en',\n",
    "                                                max_id=str(max_id - 1))\n",
    "                    else:\n",
    "                        new_tweets = self.api.search(q=query, count=tweetsPerQry,geo = '43.651070,-79.347015,250km', lang='en',\n",
    "                                                max_id=str(max_id - 1),\n",
    "                                                since_id=sinceId)\n",
    "                if not new_tweets:\n",
    "                    print(\"No more tweets found\")\n",
    "                    break\n",
    "\n",
    "                for tweet in new_tweets:\n",
    "                    parsed_tweet = {} \n",
    "                    parsed_tweet['tweets'] = tweet.text \n",
    "\n",
    "                    # appending parsed tweet to tweets list \n",
    "                    if tweet.retweet_count > 0: \n",
    "                        # if tweet has retweets, ensure that it is appended only once \n",
    "                        if parsed_tweet not in tweets: \n",
    "                            tweets.append(parsed_tweet) \n",
    "                    else: \n",
    "                        tweets.append(parsed_tweet) \n",
    "                        \n",
    "                tweetCount += len(new_tweets)\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                max_id = new_tweets[-1].id\n",
    "\n",
    "            except tweepy.TweepError as e:\n",
    "                # Just exit if any error\n",
    "                print(\"Tweepy error : \" + str(e))\n",
    "                break\n",
    "        \n",
    "        return pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 74 tweets\n",
      "Downloaded 173 tweets\n",
      "Downloaded 267 tweets\n",
      "Downloaded 367 tweets\n",
      "Downloaded 462 tweets\n",
      "Downloaded 541 tweets\n",
      "Downloaded 641 tweets\n",
      "Downloaded 732 tweets\n",
      "Downloaded 821 tweets\n",
      "Downloaded 907 tweets\n",
      "Downloaded 999 tweets\n",
      "Downloaded 1090 tweets\n",
      "Downloaded 1164 tweets\n",
      "Downloaded 1249 tweets\n",
      "Downloaded 1328 tweets\n",
      "Downloaded 1401 tweets\n",
      "Downloaded 1472 tweets\n",
      "Downloaded 1544 tweets\n",
      "Downloaded 1621 tweets\n",
      "Downloaded 1688 tweets\n",
      "Downloaded 1782 tweets\n",
      "Downloaded 1872 tweets\n",
      "Downloaded 1946 tweets\n",
      "Downloaded 2034 tweets\n",
      "Downloaded 2119 tweets\n",
      "Downloaded 2199 tweets\n",
      "Downloaded 2289 tweets\n",
      "Downloaded 2370 tweets\n",
      "Downloaded 2451 tweets\n",
      "Downloaded 2543 tweets\n",
      "Downloaded 2618 tweets\n",
      "Downloaded 2687 tweets\n",
      "Downloaded 2764 tweets\n",
      "Downloaded 2827 tweets\n",
      "Downloaded 2894 tweets\n",
      "Downloaded 2963 tweets\n",
      "Downloaded 3022 tweets\n",
      "Downloaded 3087 tweets\n",
      "Downloaded 3146 tweets\n",
      "Downloaded 3205 tweets\n",
      "Downloaded 3279 tweets\n",
      "Tweepy error : Failed to send request: HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out.\n",
      "tweets_df Shape - (644, 1)\n"
     ]
    }
   ],
   "source": [
    "twitter_client = TwitterClient()\n",
    "\n",
    "# calling function to get tweets\n",
    "tweets_df = twitter_client.get_tweets('HSBC', maxTweets=7000)\n",
    "print(f'tweets_df Shape - {tweets_df.shape}')\n",
    "# tweets_df.head(10)\n",
    "# len(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @IvanPentchoukov: Leaked files expose mass ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@stuart_onyeche @andy4wm Many companies &amp;amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @PM_Thornton: The Telegraph: Senior staff a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @benedictrogers: This may be part of the ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @JPMediaBoss: Dominion Voting Systems paten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>#xrpcommunity \\nNow that changes things.... 🤔\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>@RobTheDadBod @waltshaub @Scotus I advocated a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>RT @MissMaga2016: Boeing\\nAirbus\\nRolls Royce\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>@HSBC_UK I’m closing my HSBC bank account afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>This is not a sexy headline, and will be ignor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>644 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets\n",
       "0    RT @IvanPentchoukov: Leaked files expose mass ...\n",
       "1    @stuart_onyeche @andy4wm Many companies &amp; ...\n",
       "2    RT @PM_Thornton: The Telegraph: Senior staff a...\n",
       "3    RT @benedictrogers: This may be part of the ex...\n",
       "4    RT @JPMediaBoss: Dominion Voting Systems paten...\n",
       "..                                                 ...\n",
       "639  #xrpcommunity \\nNow that changes things.... 🤔\\...\n",
       "640  @RobTheDadBod @waltshaub @Scotus I advocated a...\n",
       "641  RT @MissMaga2016: Boeing\\nAirbus\\nRolls Royce\\...\n",
       "642  @HSBC_UK I’m closing my HSBC bank account afte...\n",
       "643  This is not a sexy headline, and will be ignor...\n",
       "\n",
       "[644 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment classification for above tweets\n",
    "\n",
    "# 1 way\n",
    "def fetch_sentiment_using_SIA(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    polarity_scores = sid.polarity_scores(text)\n",
    "    return 'neg' if polarity_scores['neg'] > polarity_scores['pos'] else 'pos'\n",
    "\n",
    "# 2 way\n",
    "def fetch_sentiment_using_textblob(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return 'pos' if analysis.sentiment.polarity >= 0 else 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in Sentiment_Analysis_updated.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main b56c6a4] New code push\n",
      " 1 file changed, 4 insertions(+), 5 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in Sentiment_Analysis_updated.ipynb.\n",
      "The file will have its original line endings in your working directory\n",
      "To https://github.com/Ashishkhurana01/NLP.git\n",
      "   abbe4f1..b56c6a4  main -> main\n"
     ]
    }
   ],
   "source": [
    "! git fetch\n",
    "! git add Sentiment_Analysis_updated.ipynb\n",
    "! git commit -m \"New code push\" Sentiment_Analysis_updated.ipynb\n",
    "! git push origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
