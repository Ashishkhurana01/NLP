{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\khura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "# for all NLP related operations on text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# To consume Twitter's API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler \n",
    "\n",
    "# To identify the sentiment of text\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "# ignoring all the warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# downloading stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('brown')\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# for showing all the plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterClient(object): \n",
    "    def __init__(self): \n",
    "        #Initialization method. \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            auth = OAuthHandler('MWagHIMZgBCE8ExJw4ZEhBeqd', 'pHoXxG9mXVDlLInne7xajFJ8WdyPKhuXdAwJOnYu9qUOT0Go8U') \n",
    "            # set access token and secret \n",
    "            auth.set_access_token('18837312-7UAgD8xarB9RgUKv263ftqGKnYkSIHAgjzVvf7K1f', 'pjc3MOSDmCXz3mAFt28gfjWdQqNVqcSUzA4BhVChNFQqP') \n",
    "            # create tweepy API object to fetch tweets \n",
    "            # add hyper parameter 'proxy' if executing from behind proxy \"proxy='http://172.22.218.218:8085'\"\n",
    "            self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "            \n",
    "        except tweepy.TweepError as e:\n",
    "            print(f\"Error: Twitter Authentication Failed - \\n{str(e)}\")\n",
    "\n",
    "    def get_tweets(self, query, maxTweets = 1000):\n",
    "        #Function to fetch tweets. \n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "        sinceId = None\n",
    "        max_id = -1\n",
    "        tweetCount = 0\n",
    "        tweetsPerQry = 100\n",
    "\n",
    "        while tweetCount < maxTweets:\n",
    "            try:\n",
    "                if (max_id <= 0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = self.api.search(q=query, count=tweetsPerQry)\n",
    "                    else:\n",
    "                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n",
    "                                                since_id=sinceId)\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n",
    "                                                max_id=str(max_id - 1))\n",
    "                    else:\n",
    "                        new_tweets = self.api.search(q=query, count=tweetsPerQry,\n",
    "                                                max_id=str(max_id - 1),\n",
    "                                                since_id=sinceId)\n",
    "                if not new_tweets:\n",
    "                    print(\"No more tweets found\")\n",
    "                    break\n",
    "\n",
    "                for tweet in new_tweets:\n",
    "                    parsed_tweet = {} \n",
    "                    parsed_tweet['tweets'] = tweet.text \n",
    "\n",
    "                    # appending parsed tweet to tweets list \n",
    "                    if tweet.retweet_count > 0: \n",
    "                        # if tweet has retweets, ensure that it is appended only once \n",
    "                        if parsed_tweet not in tweets: \n",
    "                            tweets.append(parsed_tweet) \n",
    "                    else: \n",
    "                        tweets.append(parsed_tweet) \n",
    "                        \n",
    "                tweetCount += len(new_tweets)\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                max_id = new_tweets[-1].id\n",
    "\n",
    "            except tweepy.TweepError as e:\n",
    "                # Just exit if any error\n",
    "                print(\"Tweepy error : \" + str(e))\n",
    "                break\n",
    "        \n",
    "        return pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 100 tweets\n",
      "Downloaded 198 tweets\n",
      "Downloaded 298 tweets\n",
      "Downloaded 383 tweets\n",
      "Downloaded 483 tweets\n",
      "Downloaded 580 tweets\n",
      "Downloaded 674 tweets\n",
      "Downloaded 766 tweets\n",
      "Downloaded 862 tweets\n",
      "Downloaded 954 tweets\n",
      "Downloaded 1047 tweets\n",
      "Downloaded 1138 tweets\n",
      "Downloaded 1235 tweets\n",
      "Downloaded 1329 tweets\n",
      "Downloaded 1420 tweets\n",
      "Downloaded 1497 tweets\n",
      "Downloaded 1590 tweets\n",
      "Downloaded 1676 tweets\n",
      "Downloaded 1757 tweets\n",
      "Downloaded 1836 tweets\n",
      "Downloaded 1913 tweets\n",
      "Downloaded 1973 tweets\n",
      "Downloaded 2046 tweets\n",
      "Downloaded 2123 tweets\n",
      "Downloaded 2201 tweets\n",
      "Downloaded 2271 tweets\n",
      "Downloaded 2346 tweets\n",
      "Downloaded 2437 tweets\n",
      "Downloaded 2531 tweets\n",
      "Downloaded 2618 tweets\n",
      "Downloaded 2701 tweets\n",
      "Downloaded 2782 tweets\n",
      "Downloaded 2853 tweets\n",
      "Downloaded 2937 tweets\n",
      "Downloaded 3029 tweets\n",
      "Downloaded 3113 tweets\n",
      "Downloaded 3190 tweets\n",
      "Downloaded 3275 tweets\n",
      "Downloaded 3350 tweets\n",
      "Downloaded 3425 tweets\n",
      "Downloaded 3503 tweets\n",
      "Downloaded 3581 tweets\n",
      "Downloaded 3668 tweets\n",
      "Downloaded 3768 tweets\n",
      "Downloaded 3866 tweets\n",
      "Downloaded 3963 tweets\n",
      "Downloaded 4063 tweets\n",
      "Downloaded 4163 tweets\n",
      "Downloaded 4263 tweets\n",
      "Downloaded 4359 tweets\n",
      "Downloaded 4449 tweets\n",
      "Downloaded 4529 tweets\n",
      "Downloaded 4625 tweets\n",
      "Downloaded 4719 tweets\n",
      "Downloaded 4813 tweets\n",
      "Downloaded 4900 tweets\n",
      "Downloaded 4989 tweets\n",
      "Downloaded 5087 tweets\n",
      "Downloaded 5167 tweets\n",
      "Downloaded 5251 tweets\n",
      "Downloaded 5327 tweets\n",
      "Downloaded 5396 tweets\n",
      "Downloaded 5477 tweets\n",
      "Downloaded 5558 tweets\n",
      "Downloaded 5629 tweets\n",
      "Downloaded 5701 tweets\n",
      "Downloaded 5784 tweets\n",
      "Downloaded 5860 tweets\n",
      "Downloaded 5935 tweets\n",
      "Downloaded 5998 tweets\n",
      "Downloaded 6065 tweets\n",
      "Downloaded 6132 tweets\n",
      "Downloaded 6201 tweets\n",
      "Downloaded 6264 tweets\n",
      "Downloaded 6328 tweets\n",
      "Downloaded 6399 tweets\n",
      "Downloaded 6480 tweets\n",
      "Downloaded 6544 tweets\n",
      "Downloaded 6609 tweets\n",
      "Downloaded 6681 tweets\n",
      "Downloaded 6750 tweets\n",
      "Downloaded 6824 tweets\n",
      "Downloaded 6898 tweets\n",
      "Downloaded 6950 tweets\n",
      "Downloaded 7016 tweets\n",
      "tweets_df Shape - (1108, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1108"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_client = TwitterClient()\n",
    "\n",
    "# calling function to get tweets\n",
    "tweets_df = twitter_client.get_tweets('HSBC', maxTweets=7000)\n",
    "print(f'tweets_df Shape - {tweets_df.shape}')\n",
    "tweets_df.head(10)\n",
    "len(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @yukou_takahashi: かなり濃厚な情報で満載なので、さらに抜粋・翻訳して...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Video_Forensics: Leaked files expose mass ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @silvano_trotta: Retour sur les 2 millions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @JohnHemmings2: “This is no surprise but a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@whereisindia Hello, apologize for any inconve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Merkez_Bankasi @facc_ag @invest @SocieteGener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@HSBC_UK customer for over 25 years and this b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Last British governor of HK Patten asked \\n@HS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @Stand_with_HK: Patten🇬🇧 asked @HSBC to spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @naoyafujiwara: HSBC、ドイツ銀行の上級スタッフも中国共産党員だった...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets\n",
       "0  RT @yukou_takahashi: かなり濃厚な情報で満載なので、さらに抜粋・翻訳して...\n",
       "1  RT @Video_Forensics: Leaked files expose mass ...\n",
       "2  RT @silvano_trotta: Retour sur les 2 millions ...\n",
       "3  RT @JohnHemmings2: “This is no surprise but a ...\n",
       "4  @whereisindia Hello, apologize for any inconve...\n",
       "5  @Merkez_Bankasi @facc_ag @invest @SocieteGener...\n",
       "6  @HSBC_UK customer for over 25 years and this b...\n",
       "7  Last British governor of HK Patten asked \\n@HS...\n",
       "8  RT @Stand_with_HK: Patten🇬🇧 asked @HSBC to spe...\n",
       "9  RT @naoyafujiwara: HSBC、ドイツ銀行の上級スタッフも中国共産党員だった..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in Sentiment_Analysis_updated.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main abbe4f1] New code push\n",
      " 1 file changed, 504 insertions(+), 2 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in Sentiment_Analysis_updated.ipynb.\n",
      "The file will have its original line endings in your working directory\n",
      "To https://github.com/Ashishkhurana01/NLP.git\n",
      "   454d9d6..abbe4f1  main -> main\n"
     ]
    }
   ],
   "source": [
    "! git fetch\n",
    "! git add Sentiment_Analysis_updated.ipynb\n",
    "! git commit -m \"New code push\" Sentiment_Analysis_updated.ipynb\n",
    "! git push origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
